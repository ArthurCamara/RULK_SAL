{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b7952-09e7-4e1e-9044-bd7c6f4be688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "import urllib.parse\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy.spatial.distance import cdist\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2b3da-6942-42ed-b88c-7bea90199bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model. May take a while without a GPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model = \"msmarco-MiniLM-L6-cos-v5\"\n",
    "\n",
    "bert_model = SentenceTransformer(base_model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385a36b-19b3-4a46-a0d1-66490aa60159",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_sum = {}\n",
    "golden_mean = {}\n",
    "golden_all = {}\n",
    "golden_trunc = {}\n",
    "\n",
    "for l in open(\"../data/nirmal_targets.csv\"):\n",
    "    topic_id, text = l.strip().split(\"\\t\", maxsplit=1)\n",
    "    topic_id = int(topic_id)\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = bert_model.encode(sentences, normalize_embeddings=True)\n",
    "\n",
    "    golden_trunc[topic_id] = bert_model.encode(text, normalize_embeddings=True)\n",
    "    golden_sum[topic_id] = np.sum(embeddings, axis=0)\n",
    "    golden_mean[topic_id] = np.mean(embeddings, axis=0)\n",
    "    golden_all[topic_id] = embeddings\n",
    "\n",
    "pickle.dump(golden_mean, open(f\"../data/bert_embeddings/{base_model}_golden_mean_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(golden_sum, open(f\"../data/bert_embeddings/{base_model}_golden_sum_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(golden_all, open(f\"../data/bert_embeddings/{base_model}_golden_all_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(golden_trunc, open(f\"../data/bert_embeddings/{base_model}_golden_trunc_embeddings.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e53573-c08a-46e0-bd4b-4a9fc2a0f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_sum = {}\n",
    "docs_mean = {}\n",
    "docs_trunc = {}\n",
    "\n",
    "for line in tqdm(open(\"../data/clicked_docs_nirmal.tsv\"), total=383):\n",
    "    try:\n",
    "        url, topic, text = line.strip().split(\"\\t\", maxsplit=2)\n",
    "    except ValueError:\n",
    "        url = line.split(\"\\t\")[0]\n",
    "        docs_sum[url] = np.zeros(bert_model[1].word_embedding_dimension)  # Empty. We don't have this embedding now.\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    embeddings = bert_model.encode(sentences, normalize_embeddings=True)\n",
    "    docs_trunc[url] = bert_model.encode(text, normalize_embeddings=True)\n",
    "\n",
    "    docs_sum[url] = np.sum(embeddings, axis=0)\n",
    "    docs_mean[url] = np.mean(embeddings, axis=0)\n",
    "\n",
    "pickle.dump(docs_mean, open(f\"../data/bert_embeddings/{base_model}nirmal_docs_mean_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(docs_sum, open(f\"../data/bert_embeddings/{base_model}nirmal_docs_sum_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(docs_trunc, open(f\"../data/bert_embeddings/{base_model}nirmal_docs_trunc_embeddings.pkl\", \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESIRES",
   "language": "python",
   "name": "desires"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
