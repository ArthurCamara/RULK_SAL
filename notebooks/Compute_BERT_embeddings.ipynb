{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11e405d-28d3-4d7b-b2fc-76e7fe973b25",
   "metadata": {},
   "source": [
    "# Compute Bert Embeddings\n",
    "There will be a lot of BERT embeddings to be computed. It's easier to pre-compute all of them, and them use what we ACTUALLY need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06738c0-39e3-4dd1-868d-bdb24ea4e75a",
   "metadata": {},
   "source": [
    "## Embeddings for Wikipedia pages\n",
    "Wikipedia pages can be too long. So we need a strategy for pooling such long documents.\n",
    "we are using the [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) model as base, using a mean pooling strategy from `sentence-transformers` to compute embeddings.\n",
    "\n",
    "Each document is split into sentences, using NLTK. If a sentence is longer than the limit of the model (384 tokens in this case), we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dd2da9e-3944-4b88-b201-29fdcf7881bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "\n",
    "def normalize_vector(v):\n",
    "    return v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed19b4a-6c17-43a4-9f6e-495b56f637b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model. May take a while without a GPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bert_model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f81e0aca-2a11-4244-b032-eb265fb58f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_sum = {}\n",
    "wikipedia_mean = {}\n",
    "wikipedia_all = {}\n",
    "\n",
    "for l in open(\"../data/wikipedia_texts.tsv\"):\n",
    "    topic_title, text = l.strip().split(\"\\t\", maxsplit=1)\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = bert_model.encode(sentences, normalize_embeddings=True)\n",
    "    wikipedia_sum[topic_title] = normalize_vector(np.sum(embeddings, axis=0))\n",
    "    wikipedia_mean[topic_title] = normalize_vector(np.mean(embeddings, axis=0))\n",
    "    wikipedia_all[topic_title] = embeddings\n",
    "\n",
    "# dump to a pickle file\n",
    "pickle.dump(wikipedia_mean, open(\"wikipedia_mean_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_sum, open(\"wikipedia_sum_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_all, open(\"wikipedia_all_embeddings.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6f522-b024-4648-85f2-2fe7903c11a4",
   "metadata": {},
   "source": [
    "## Embeddings for clicked documents\n",
    "Clicked documents can also be too long. Here we use three different approaches. Pooling (with mean/sum) and a BIRCH-like one, where we only keep the sentence with higher similarity to the \"golden\" ALL of the embeddings, and, when computing similarity, we only keep the one with higher score to the golden  pages can be too long. So we need a strategy for pooling such long documents. We will try the following:\n",
    "\n",
    "- Mean of embeddings vs mean of wikipedia\n",
    "- Sum of embeddings vs mean of wikipedia\n",
    "- BIRCH MaxP all vs all: Keep Best score (max) for each sentence vs each sentence from wikipedia\n",
    "- BIRCH MaxP all vs SUM: Keep Best score (max) for each sentence vs SUM of wikipedia\n",
    "- BIRCH MaxP all vs MEAN: Keep Best score (max) for each sentence vs MEAN of wikipedia\n",
    "\n",
    "we are using the [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) model as base, using a mean pooling strategy from `sentence-transformers` to compute embeddings.\n",
    "\n",
    "Each document is split into sentences, using NLTK. If a sentence is longer than the limit of the model (384 tokens in this case), we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c90f243b-b130-4fd9-bdfb-6b5ee7cddde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "947 ../data/clicked_docs_with_topics.tsv\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../data/clicked_docs_with_topics.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24f99f01-851c-4165-a74b-cc86f2fb5d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dd104cae3241578b198efbb5e4eeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/947 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs_sum = {}\n",
    "docs_mean = {}\n",
    "maxp_pairwise = {}\n",
    "maxp_sum = {}\n",
    "maxp_mean = {}\n",
    "\n",
    "for line in tqdm(open(\"../data/clicked_docs_with_topics.tsv\"), total=947):\n",
    "    url, topic, text = line.split(\"\\t\", maxsplit=2)\n",
    "    topic = \n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = bert_model.encode(sentences, normalize_embeddings=True)\n",
    "    docs_sum[url] = normalize_vector(np.sum(embeddings, axis=0))\n",
    "    docs_mean[url] = normalize_vector(np.mean(embeddings, axis=0))\n",
    "\n",
    "    # GET PAIRWISE data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12dfbbf2-0a02-4c36-9f2d-908e48f2f5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Subprime%20mortgage%20crisis', 'Irritable%20bowel%20syndrome', 'Genetically%20modified%20organism', 'Noise-induced%20hearing%20loss', 'Business%20cycle', 'Ethics', 'Radiocarbon%20dating%20considerations'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_all.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESIRES",
   "language": "python",
   "name": "desires"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "12dd104cae3241578b198efbb5e4eeef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_de69d51b51dc47c8a626b766b414fa61",
        "IPY_MODEL_9e4d4e73efd24a0d901e1a17ea0815a4",
        "IPY_MODEL_db911ddb2d9640db88a846ec3bcb9e5e"
       ],
       "layout": "IPY_MODEL_5b89f0b2e79f4a08989f23f9aa583f15"
      }
     },
     "1e93dad1519b4eaca9ac7b1e2330344c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1fca2f9da56e4d919daf49dd6fc4365c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2513fab62f9042c2b6c570ccca56b68c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "29b247f7f7da4265bac2790876d1f431": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8c733073f35b4e3c8bb4e0f1987ae583",
       "style": "IPY_MODEL_b95a95db8fe148c79d0eacec13bf07ee",
       "value": "  0%"
      }
     },
     "2b3d6e4e615f42ca8f174fd9cb389e16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2b3e1d7618d54c27b087fdc2ce755612": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_dda8034b3fe8482698a0f1dd0163c033",
       "max": 947,
       "style": "IPY_MODEL_e4433f03af0049afa51c047afd2236c2"
      }
     },
     "39b757d801264051b82127fef77087e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_29b247f7f7da4265bac2790876d1f431",
        "IPY_MODEL_2b3e1d7618d54c27b087fdc2ce755612",
        "IPY_MODEL_e612c2c481a0401db233964192dba82e"
       ],
       "layout": "IPY_MODEL_4139e505d8b64d45aa75863f1b93d0cc"
      }
     },
     "4139e505d8b64d45aa75863f1b93d0cc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5b89f0b2e79f4a08989f23f9aa583f15": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "64510559c8b543b3b89b94ec64e93339": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6da813fe650c4cb19be62659b32a9b82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7c73075e2b3e4d02a89a7aa8189e97e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c733073f35b4e3c8bb4e0f1987ae583": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e4d4e73efd24a0d901e1a17ea0815a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_2513fab62f9042c2b6c570ccca56b68c",
       "max": 947,
       "style": "IPY_MODEL_2b3d6e4e615f42ca8f174fd9cb389e16"
      }
     },
     "b95a95db8fe148c79d0eacec13bf07ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "db911ddb2d9640db88a846ec3bcb9e5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ddd8e6b140df4e35a803c26220523ce0",
       "style": "IPY_MODEL_64510559c8b543b3b89b94ec64e93339",
       "value": " 0/947 [00:00&lt;?, ?it/s]"
      }
     },
     "dda8034b3fe8482698a0f1dd0163c033": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ddd8e6b140df4e35a803c26220523ce0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "de69d51b51dc47c8a626b766b414fa61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7c73075e2b3e4d02a89a7aa8189e97e2",
       "style": "IPY_MODEL_6da813fe650c4cb19be62659b32a9b82",
       "value": "  0%"
      }
     },
     "e4433f03af0049afa51c047afd2236c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e612c2c481a0401db233964192dba82e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1fca2f9da56e4d919daf49dd6fc4365c",
       "style": "IPY_MODEL_1e93dad1519b4eaca9ac7b1e2330344c",
       "value": " 0/947 [00:00&lt;?, ?it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
