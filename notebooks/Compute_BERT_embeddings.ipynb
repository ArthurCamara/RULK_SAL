{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11e405d-28d3-4d7b-b2fc-76e7fe973b25",
   "metadata": {},
   "source": [
    "# Compute Bert Embeddings\n",
    "There will be a lot of BERT embeddings to be computed. It's easier to pre-compute all of them, and them use what we ACTUALLY need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06738c0-39e3-4dd1-868d-bdb24ea4e75a",
   "metadata": {},
   "source": [
    "## Embeddings for Wikipedia pages\n",
    "Wikipedia pages can be too long. So we need a strategy for pooling such long documents.\n",
    "we are using the [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) model as base, using a mean pooling strategy from `sentence-transformers` to compute embeddings.\n",
    "\n",
    "Each document is split into sentences, using NLTK. If a sentence is longer than the limit of the model (384 tokens in this case), we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd2da9e-3944-4b88-b201-29fdcf7881bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "\n",
    "def normalize_vector(v):\n",
    "    return v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ed19b4a-6c17-43a4-9f6e-495b56f637b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model. May take a while without a GPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bert_model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f81e0aca-2a11-4244-b032-eb265fb58f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_sum = {}\n",
    "wikipedia_mean = {}\n",
    "\n",
    "for l in open(\"../data/wikipedia_texts.tsv\"):\n",
    "    topic_title, text = l.strip().split(\"\\t\", maxsplit=1)\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = bert_model.encode(sentences, normalize_embeddings=True)\n",
    "    wikipedia_sum[topic_title] = normalize_vector(np.sum(embeddings, axis=0))\n",
    "    wikipedia_mean[topic_title] = normalize_vector(np.mean(embeddings, axis=0))\n",
    "\n",
    "# dump to a pickle file\n",
    "pickle.dump(wikipedia_mean, open(\"wikipedia_mean_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_sum, open(\"wikipedia_sum_embeddings.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6f522-b024-4648-85f2-2fe7903c11a4",
   "metadata": {},
   "source": [
    "## Embeddings for clicked documents\n",
    "Clicked documents can also be too long. Here we use three different approaches. Pooling (with mean/sum) and a BIRCH-like one, where we only keep the sentence with higher similarity to the \"golden\" ALL of the embeddings, and, when computing similarity, we only keep the one with higher score to the golden  pages can be too long. So we need a strategy for pooling such long documents.\n",
    "we are using the [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) model as base, using a mean pooling strategy from `sentence-transformers` to compute embeddings.\n",
    "\n",
    "Each document is split into sentences, using NLTK. If a sentence is longer than the limit of the model (384 tokens in this case), we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f99f01-851c-4165-a74b-cc86f2fb5d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESIRES",
   "language": "python",
   "name": "desires"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
