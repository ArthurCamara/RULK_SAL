{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11e405d-28d3-4d7b-b2fc-76e7fe973b25",
   "metadata": {},
   "source": [
    "# Compute Bert Embeddings\n",
    "There will be a lot of BERT embeddings to be computed. It's easier to pre-compute all of them, and them use what we ACTUALLY need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06738c0-39e3-4dd1-868d-bdb24ea4e75a",
   "metadata": {},
   "source": [
    "## Embeddings for Wikipedia pages\n",
    "Wikipedia pages can be too long. So we need a strategy for pooling such long documents.\n",
    "we are using the [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) model as base, using a mean pooling strategy from `sentence-transformers` to compute embeddings.\n",
    "\n",
    "Each document is split into sentences, using NLTK. If a sentence is longer than the limit of the model (384 tokens in this case), we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dd2da9e-3944-4b88-b201-29fdcf7881bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import urllib.parse\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "def normalize_vector(v):\n",
    "    return v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ed19b4a-6c17-43a4-9f6e-495b56f637b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model. May take a while without a GPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bert_model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bf297611-052e-4d08-8418-eb4152849432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Radiocarbon%2520dating%2520considerations'])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_trunc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "91a74f6e-7ce0-44fc-9db8-0406dd9c8420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Subprime%20mortgage%20crisis', 'Irritable%20bowel%20syndrome', 'Genetically%20modified%20organism', 'Noise-induced%20hearing%20loss', 'Business%20cycle', 'Ethics', 'Radiocarbon%20dating%20considerations'])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_sum.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9b566e82-8a8e-4e2f-9ba4-8e647f610724",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Subprime%20mortgage%20crisis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [201]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwikipedia_trunc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSubprime\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m20mortgage\u001b[39;49m\u001b[38;5;132;43;01m%20c\u001b[39;49;00m\u001b[38;5;124;43mrisis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Subprime%20mortgage%20crisis'"
     ]
    }
   ],
   "source": [
    "wikipedia_trunc[\"Subprime%20mortgage%20crisis\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f81e0aca-2a11-4244-b032-eb265fb58f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_sum = {}\n",
    "wikipedia_mean = {}\n",
    "wikipedia_all = {}\n",
    "wikipedia_trunc = {}\n",
    "\n",
    "for l in open(\"../data/wikipedia_texts.tsv\"):\n",
    "    topic_title, text = l.strip().split(\"\\t\", maxsplit=1)\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = bert_model.encode(sentences, normalize_embeddings=True)\n",
    "    wikipedia_sum[topic_title] = normalize_vector(np.sum(embeddings, axis=0))\n",
    "    wikipedia_mean[topic_title] = normalize_vector(np.mean(embeddings, axis=0))\n",
    "    wikipedia_trunc[topic_title] = bert_model.encode(text)\n",
    "\n",
    "    wikipedia_all[topic_title] = embeddings\n",
    "\n",
    "# dump to a pickle file\n",
    "pickle.dump(wikipedia_mean, open(\"../data/wikipedia_mean_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_sum, open(\"../data/wikipedia_sum_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_all, open(\"../data/wikipedia_all_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_trunc, open(\"../data/wikipedia_trunc_embeddings.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6f522-b024-4648-85f2-2fe7903c11a4",
   "metadata": {},
   "source": [
    "## Embeddings for clicked documents\n",
    "Clicked documents can also be too long. Here we use three different approaches. Pooling (with mean/sum) and a BIRCH-like one, where we only keep the sentence with higher similarity to the \"golden\" ALL of the embeddings, and, when computing similarity, we only keep the one with higher score to the golden  pages can be too long. So we need a strategy for pooling such long documents. We will try the following:\n",
    "\n",
    "- Mean of embeddings vs mean of wikipedia\n",
    "- Sum of embeddings vs mean of wikipedia\n",
    "- Truncate doc at maximum length (384)\n",
    "- BIRCH MaxP all vs all: Keep Best score (max) for each sentence vs each sentence from wikipedia\n",
    "- BIRCH MaxP all vs SUM: Keep Best score (max) for each sentence vs SUM of wikipedia\n",
    "- BIRCH MaxP all vs MEAN: Keep Best score (max) for each sentence vs MEAN of wikipedia\n",
    "\n",
    "we are using the [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) model as base, using a mean pooling strategy from `sentence-transformers` to compute embeddings.\n",
    "\n",
    "Each document is split into sentences, using NLTK. If a sentence is longer than the limit of the model (384 tokens in this case), we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "24f99f01-851c-4165-a74b-cc86f2fb5d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1521db96ea924aeea3e5b96beb317e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/947 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs_sum = {}\n",
    "docs_mean = {}\n",
    "docs_trunc = {}\n",
    "maxp_pairwise = {}\n",
    "maxp_sum = {}\n",
    "maxp_mean = {}\n",
    "maxp_trunc = {}\n",
    "\n",
    "for line in tqdm(open(\"../data/clicked_docs_with_topics.tsv\"), total=947):\n",
    "    try:\n",
    "        url, topic, text = line.strip().split(\"\\t\", maxsplit=2)\n",
    "    except ValueError:\n",
    "        url = line.split(\"\\t\")[0]\n",
    "        docs_sum[url] = np.zeros(768)  # Empty. We don't have this embedding now.\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = bert_model.encode(sentences, normalize_embeddings=True)\n",
    "    docs_sum[url] = normalize_vector(np.sum(embeddings, axis=0))\n",
    "    docs_mean[url] = normalize_vector(np.mean(embeddings, axis=0))\n",
    "    docs_trunc[url] = bert_model.encode(text, normalize_embeddings=True)\n",
    "\n",
    "    # Best PAIRWISE similarity\n",
    "    wikipedia_embeddings = wikipedia_all[topic]\n",
    "    # dimensions: (document_embeddings, wikipedia_embeddings)\n",
    "    pairwise_similarities = np.dot(embeddings, wikipedia_embeddings.T)\n",
    "    best_doc, best_wiki = np.unravel_index(pairwise_similarities.argmax(), pairwise_similarities.shape)\n",
    "\n",
    "    maxp_pairwise[url] = embeddings[best_doc]\n",
    "    maxp_sum[url] = embeddings[np.argmax(np.dot(embeddings, wikipedia_sum[topic].T))]\n",
    "    maxp_mean[url] = embeddings[np.argmax(np.dot(embeddings, wikipedia_mean[topic].T))]\n",
    "    maxp_trunc[url] = embeddings[np.argmax(np.dot(embeddings, wikipedia_trunc[topic].T))]\n",
    "\n",
    "# dump to a pickle file\n",
    "pickle.dump(docs_mean, open(\"../data/docs_mean_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(docs_sum, open(\"../data/docs_sum_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(docs_trunc, open(\"../data/docs_trunc_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_pairwise, open(\"../data/docs_maxp_pairwise_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_sum, open(\"../data/docs_maxp_sum_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_mean, open(\"../data/docs_maxp_mean_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_trunc, open(\"../data/docs_maxp_trunc_embeddings.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1b853-6cb1-456e-8b70-38e647b83ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESIRES",
   "language": "python",
   "name": "desires"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
