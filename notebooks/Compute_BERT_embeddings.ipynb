{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11e405d-28d3-4d7b-b2fc-76e7fe973b25",
   "metadata": {},
   "source": [
    "# Compute Bert Embeddings\n",
    "There will be a lot of BERT embeddings to be computed. It's easier to pre-compute all of them, and them use what we ACTUALLY need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06738c0-39e3-4dd1-868d-bdb24ea4e75a",
   "metadata": {},
   "source": [
    "## Embeddings for Wikipedia pages\n",
    "Wikipedia pages can be too long. So we need a strategy for pooling such long documents.\n",
    "we are using the [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) model as base, using a mean pooling strategy from `sentence-transformers` to compute embeddings.\n",
    "\n",
    "Each document is split into sentences, using NLTK. If a sentence is longer than the limit of the model (384 tokens in this case), we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd2da9e-3944-4b88-b201-29fdcf7881bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T08:23:18.785372Z",
     "iopub.status.busy": "2022-06-15T08:23:18.784407Z",
     "iopub.status.idle": "2022-06-15T08:23:18.795873Z",
     "shell.execute_reply": "2022-06-15T08:23:18.794769Z",
     "shell.execute_reply.started": "2022-06-15T08:23:18.785311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "import urllib.parse\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy.spatial.distance import cdist\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed19b4a-6c17-43a4-9f6e-495b56f637b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T08:23:19.327333Z",
     "iopub.status.busy": "2022-06-15T08:23:19.327009Z",
     "iopub.status.idle": "2022-06-15T08:23:25.858360Z",
     "shell.execute_reply": "2022-06-15T08:23:25.857631Z",
     "shell.execute_reply.started": "2022-06-15T08:23:19.327301Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare model. May take a while without a GPU.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# base_model = \"all-MiniLM-L6-v2\"\n",
    "base_model = \"msmarco-MiniLM-L6-cos-v5\"\n",
    "# base_model = \"all-mpnet-base-v2\"\n",
    "\n",
    "bert_model = SentenceTransformer(base_model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f81e0aca-2a11-4244-b032-eb265fb58f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T08:23:45.790461Z",
     "iopub.status.busy": "2022-06-15T08:23:45.789895Z",
     "iopub.status.idle": "2022-06-15T08:23:46.272496Z",
     "shell.execute_reply": "2022-06-15T08:23:46.271637Z",
     "shell.execute_reply.started": "2022-06-15T08:23:45.790405Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wikipedia_sum = {}\n",
    "wikipedia_mean = {}\n",
    "wikipedia_all = {}\n",
    "wikipedia_trunc = {}\n",
    "\n",
    "wikipedia_sum_norm = {}\n",
    "wikipedia_mean_norm = {}\n",
    "wikipedia_all_norm = {}\n",
    "wikipedia_trunc_norm = {}\n",
    "\n",
    "for l in open(\"../data/wikipedia_texts.tsv\"):\n",
    "    topic_title, text = l.strip().split(\"\\t\", maxsplit=1)\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = bert_model.encode(sentences, normalize_embeddings=False)\n",
    "    embeddings_norm = normalize(embeddings)\n",
    "\n",
    "    wikipedia_trunc[topic_title] = bert_model.encode(text)\n",
    "    wikipedia_trunc_norm[topic_title] = normalize(wikipedia_trunc[topic_title].reshape(1, -1)).flatten()\n",
    "\n",
    "    wikipedia_sum[topic_title] = np.sum(embeddings, axis=0)\n",
    "    wikipedia_mean[topic_title] = np.mean(embeddings, axis=0)\n",
    "\n",
    "    wikipedia_sum_norm[topic_title] = normalize(np.sum(embeddings_norm, axis=0).reshape(1, -1)).flatten()\n",
    "    wikipedia_mean_norm[topic_title] = normalize(np.mean(embeddings_norm, axis=0).reshape(1, -1)).flatten()\n",
    "\n",
    "    wikipedia_all[topic_title] = embeddings\n",
    "    wikipedia_all_norm[topic_title] = embeddings_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85bade7e-9e93-4915-8314-2738de7a52b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T08:23:46.274544Z",
     "iopub.status.busy": "2022-06-15T08:23:46.274268Z",
     "iopub.status.idle": "2022-06-15T08:23:46.288607Z",
     "shell.execute_reply": "2022-06-15T08:23:46.287706Z",
     "shell.execute_reply.started": "2022-06-15T08:23:46.274516Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dump to a pickle file\n",
    "pickle.dump(wikipedia_mean, open(f\"../data/bert_embeddings/{base_model}_wikipedia_mean_embeddings_un.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_sum, open(f\"../data/bert_embeddings/{base_model}_wikipedia_sum_embeddings_un.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_all, open(f\"../data/bert_embeddings/{base_model}_wikipedia_all_embeddings_un.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_trunc, open(f\"../data/bert_embeddings/{base_model}_wikipedia_trunc_embeddings_un.pkl\", \"wb\"))\n",
    "\n",
    "pickle.dump(wikipedia_mean_norm, open(f\"../data/bert_embeddings/{base_model}_wikipedia_mean_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_sum_norm, open(f\"../data/bert_embeddings/{base_model}_wikipedia_sum_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_all_norm, open(f\"../data/bert_embeddings/{base_model}_wikipedia_all_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(wikipedia_trunc_norm, open(f\"../data/bert_embeddings/{base_model}_wikipedia_trunc_embeddings.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6f522-b024-4648-85f2-2fe7903c11a4",
   "metadata": {},
   "source": [
    "## Embeddings for clicked documents\n",
    "Clicked documents can also be too long. Here we use three different approaches. Pooling (with mean/sum) and a BIRCH-like one, where we only keep the sentence with higher similarity to the \"golden\" ALL of the embeddings, and, when computing similarity, we only keep the one with higher score to the golden  pages can be too long. So we need a strategy for pooling such long documents. We will try the following:\n",
    "\n",
    "- Mean of embeddings vs mean of wikipedia\n",
    "- Sum of embeddings vs mean of wikipedia\n",
    "- Truncate doc at maximum length (384)\n",
    "- BIRCH MaxP all vs all: Keep Best score (max) for each sentence vs each sentence from wikipedia\n",
    "- BIRCH MaxP all vs SUM: Keep Best score (max) for each sentence vs SUM of wikipedia\n",
    "- BIRCH MaxP all vs MEAN: Keep Best score (max) for each sentence vs MEAN of wikipedia\n",
    "\n",
    "we are using the [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) model as base, using a mean pooling strategy from `sentence-transformers` to compute embeddings.\n",
    "\n",
    "Each document is split into sentences, using NLTK. If a sentence is longer than the limit of the model (384 tokens in this case), we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f99f01-851c-4165-a74b-cc86f2fb5d47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T08:24:34.945099Z",
     "iopub.status.busy": "2022-06-15T08:24:34.944526Z",
     "iopub.status.idle": "2022-06-15T08:26:01.454520Z",
     "shell.execute_reply": "2022-06-15T08:26:01.452607Z",
     "shell.execute_reply.started": "2022-06-15T08:24:34.945042Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca636a814c14995abc56a7274d13cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs_sum = {}\n",
    "docs_mean = {}\n",
    "docs_trunc = {}\n",
    "# maxp_pairwise = {}\n",
    "# maxp_sum = {}\n",
    "# maxp_mean = {}\n",
    "# maxp_trunc = {}\n",
    "\n",
    "docs_sum_norm = {}\n",
    "docs_mean_norm = {}\n",
    "docs_trunc_norm = {}\n",
    "# maxp_pairwise_norm = {}\n",
    "# maxp_sum_norm = {}\n",
    "# maxp_mean_norm = {}\n",
    "# maxp_trunc_norm = {}\n",
    "\n",
    "for line in tqdm(open(\"../data/clicked_docs_with_topics_more.tsv\"), total=1137):\n",
    "    # for line in tqdm(open(\"../data/clicked_docs.tsv\"), total=947):\n",
    "    try:\n",
    "        url, topic, text = line.strip().split(\"\\t\", maxsplit=2)\n",
    "    except ValueError:\n",
    "        url = line.split(\"\\t\")[0]\n",
    "        docs_sum[url] = np.zeros(bert_model[1].word_embedding_dimension)  # Empty. We don't have this embedding now.\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    embeddings = bert_model.encode(sentences, normalize_embeddings=False)\n",
    "    embeddings_norm = normalize(embeddings)\n",
    "\n",
    "    docs_trunc[url] = bert_model.encode(text, normalize_embeddings=False)\n",
    "    docs_trunc_norm[url] = normalize(docs_trunc[url].reshape(1, -1)).flatten()\n",
    "\n",
    "    docs_sum[url] = np.sum(embeddings, axis=0)\n",
    "    docs_mean[url] = np.mean(embeddings, axis=0)\n",
    "\n",
    "    docs_sum_norm[url] = normalize(np.sum(embeddings, axis=0).reshape(1, -1)).flatten()\n",
    "    docs_mean_norm[url] = normalize(np.mean(embeddings, axis=0).reshape(1, -1)).flatten()\n",
    "\n",
    "    # wikipedia_embeddings = wikipedia_all[topic]\n",
    "    # wikipedia_embeddings_norm = wikipedia_all_norm[topic]\n",
    "\n",
    "    # dimensions: (document_embeddings, wikipedia_embeddings)\n",
    "    # pairwise_distances = cdist(embeddings, wikipedia_embeddings, metric=\"cosine\")\n",
    "    # best_doc, _ = np.unravel_index(pairwise_distances.argmin(), pairwise_distances.shape)\n",
    "    # maxp_pairwise[url] = embeddings[best_doc]\n",
    "\n",
    "    # maxp_sum[url] = embeddings[np.argmin(cdist(embeddings, [wikipedia_sum[topic]]))]\n",
    "    # maxp_mean[url] = embeddings[np.argmin(cdist(embeddings, [wikipedia_mean[topic]]))]\n",
    "    # maxp_trunc[url] = embeddings[np.argmin(cdist(embeddings, [wikipedia_trunc[topic]]))]\n",
    "\n",
    "    # normalized version\n",
    "    # pairwise_distances = cdist(embeddings_norm, wikipedia_embeddings_norm, metric=\"cosine\")\n",
    "    # best_doc, _ = np.unravel_index(pairwise_distances.argmin(), pairwise_distances.shape)\n",
    "    # maxp_pairwise_norm[url] = embeddings_norm[best_doc]\n",
    "\n",
    "    # maxp_sum_norm[url] = embeddings_norm[np.argmin(cdist(embeddings_norm, [wikipedia_sum_norm[topic]]))]\n",
    "    # maxp_mean_norm[url] = embeddings_norm[np.argmin(cdist(embeddings_norm, [wikipedia_mean_norm[topic]]))]\n",
    "    # maxp_trunc_norm[url] = embeddings_norm[np.argmin(cdist(embeddings_norm, [wikipedia_trunc_norm[topic]]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a7f3f3-a886-4c16-b7fd-6617a582f02f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T08:28:55.542225Z",
     "iopub.status.busy": "2022-06-15T08:28:55.541515Z",
     "iopub.status.idle": "2022-06-15T08:28:55.605774Z",
     "shell.execute_reply": "2022-06-15T08:28:55.604953Z",
     "shell.execute_reply.started": "2022-06-15T08:28:55.542168Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(docs_mean, open(f\"../data/bert_embeddings/{base_model}_docs_mean_embeddings_un_more.pkl\", \"wb\"))\n",
    "pickle.dump(docs_sum, open(f\"../data/bert_embeddings/{base_model}_docs_sum_embeddings_un_more.pkl\", \"wb\"))\n",
    "pickle.dump(docs_trunc, open(f\"../data/bert_embeddings/{base_model}_docs_trunc_embeddings_un_more.pkl\", \"wb\"))\n",
    "# pickle.dump(maxp_pairwise, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_pairwise_embeddings_more.pkl\", \"wb\"))\n",
    "# pickle.dump(maxp_sum, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_sum_embeddings_more.pkl\", \"wb\"))\n",
    "# pickle.dump(maxp_mean, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_mean_embeddings_more.pkl\", \"wb\"))\n",
    "# pickle.dump(maxp_trunc, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_trunc_embeddings_more.pkl\", \"wb\"))\n",
    "\n",
    "pickle.dump(docs_mean_norm, open(f\"../data/bert_embeddings/{base_model}_docs_mean_embeddings_more.pkl\", \"wb\"))\n",
    "pickle.dump(docs_sum_norm, open(f\"../data/bert_embeddings/{base_model}_docs_sum_embeddings_more.pkl\", \"wb\"))\n",
    "pickle.dump(docs_trunc_norm, open(f\"../data/bert_embeddings/{base_model}_docs_trunc_embeddings_more.pkl\", \"wb\"))\n",
    "# pickle.dump(maxp_pairwise_norm, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_pairwise_embeddings_un_more.pkl\", \"wb\"))\n",
    "# pickle.dump(maxp_sum_norm, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_sum_embeddings_un_more.pkl\", \"wb\"))\n",
    "# pickle.dump(maxp_mean_norm, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_mean_embeddings_un_more.pkl\", \"wb\"))\n",
    "# pickle.dump(maxp_trunc_norm, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_trunc_embeddings_un_more.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eb84fb27-8386-4f26-970f-02708d54a8c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T12:34:46.801633Z",
     "iopub.status.busy": "2022-06-10T12:34:46.800992Z",
     "iopub.status.idle": "2022-06-10T12:34:46.941945Z",
     "shell.execute_reply": "2022-06-10T12:34:46.940936Z",
     "shell.execute_reply.started": "2022-06-10T12:34:46.801567Z"
    }
   },
   "outputs": [],
   "source": [
    "# dump to a pickle file\n",
    "pickle.dump(docs_mean, open(f\"../data/bert_embeddings/{base_model}_docs_mean_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(docs_sum, open(f\"../data/bert_embeddings/{base_model}_docs_sum_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(docs_trunc, open(f\"../data/bert_embeddings/{base_model}_docs_trunc_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_pairwise, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_pairwise_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_sum, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_sum_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_mean, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_mean_embeddings.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_trunc, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_trunc_embeddings.pkl\", \"wb\"))\n",
    "\n",
    "pickle.dump(docs_mean_norm, open(f\"../data/bert_embeddings/{base_model}_docs_mean_embeddings_un.pkl\", \"wb\"))\n",
    "pickle.dump(docs_sum_norm, open(f\"../data/bert_embeddings/{base_model}_docs_sum_embeddings_un.pkl\", \"wb\"))\n",
    "pickle.dump(docs_trunc_norm, open(f\"../data/bert_embeddings/{base_model}_docs_trunc_embeddings_un.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_pairwise_norm, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_pairwise_embeddings_un.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_sum_norm, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_sum_embeddings_un.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_mean_norm, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_mean_embeddings_un.pkl\", \"wb\"))\n",
    "pickle.dump(maxp_trunc_norm, open(f\"../data/bert_embeddings/{base_model}_docs_maxp_trunc_embeddings_un.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7e097-6dab-47c7-be83-d8efabff2f19",
   "metadata": {},
   "source": [
    "## Alternative: A cross-encoder model that predicts relevance (0-1)\n",
    "Final score is either SUM or MEAN of relevances for all clicked docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c90120-e6c4-4f29-966f-da623a87bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_texts = {}\n",
    "wiki_sentences = {}\n",
    "for l in open(\"../data/wikipedia_texts.tsv\"):\n",
    "    topic_title, text = l.strip().split(\"\\t\", maxsplit=1)\n",
    "    wiki_texts[topic_title] = text\n",
    "    wiki_sentences[topic_title] = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8f39a7d-d185-4a66-bda9-a99e27fa2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "base_model = \"cross-encoder/stsb-roberta-base\"\n",
    "# base_model = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "# bert_model = CrossEncoder(base_model)\n",
    "sim_matrixes = {}  # similarity between a doc and its topic\n",
    "both_truncated_scores = {}  # truncated similarity\n",
    "doc_truncated_scores = {}\n",
    "wiki_truncated_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4171d2-4096-4035-aa0b-6d304c068068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b8b52-7849-4842-9b8e-fe0d07bc4c24",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_sum = {}\n",
    "docs_mean = {}\n",
    "docs_trunc = {}\n",
    "maxp_pairwise = {}\n",
    "maxp_sum = {}\n",
    "maxp_mean = {}\n",
    "maxp_trunc = {}\n",
    "batch_size = 128\n",
    "\n",
    "for line in tqdm(open(\"../data/clicked_docs_with_topics.tsv\"), total=947):\n",
    "    try:\n",
    "        url, topic, text = line.strip().split(\"\\t\", maxsplit=2)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    sentences = sent_tokenize(text)\n",
    "    wiki_text = wiki_texts[topic]\n",
    "    wiki_sentence = wiki_sentences[topic]\n",
    "    pairs_of_sentences = list(itertools.product(sentences, wiki_sentence))\n",
    "    wiki_truncated = list(itertools.product(sentences, [wiki_text]))\n",
    "    doc_truncated = list(itertools.product([text], wiki_sentence))\n",
    "\n",
    "    scores = bert_model.predict(pairs_of_sentences, show_progress_bar=True, batch_size=batch_size)\n",
    "    scores = scores.reshape((len(sentences), len(wiki_sentence)))\n",
    "\n",
    "    scores_trunc_doc = bert_model.predict(doc_truncated, batch_size=batch_size)\n",
    "    scores_trunc_wiki = bert_model.predict(wiki_truncated, batch_size=batch_size)\n",
    "\n",
    "    sim_matrixes[url] = scores\n",
    "    both_truncated_scores[url] = bert_model.predict([text, wiki_text])\n",
    "    doc_truncated_scores[url] = scores_trunc_doc\n",
    "    wiki_truncated_scores[url] = scores_trunc_wiki\n",
    "    continue\n",
    "\n",
    "    docs_mean[url] = np.mean(scores)\n",
    "\n",
    "    # MAX-P approaches\n",
    "    # We can combine each axis of the scores matrix in different ways. Using mean, max or sum. Try all.\n",
    "\n",
    "    maxp_pairwise[url] = np.max(scores)\n",
    "    mean_sentence_score = np.mean(scores, axis=1)  # one score for each sentence\n",
    "    sum_sentence_score = np.sum(scores, axis=1)  # one score for each sentence\n",
    "    max_sentence_score = np.max(scores, axis=1)\n",
    "\n",
    "    maxp_mean[url] = np.max(mean_sentence_score)\n",
    "    maxp_sum[url] = np.max(sum_sentence_score)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ad321-7811-4699-83f1-671c741a3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sim_matrixes, open(f\"../data/{base_model}_CE_scores.pkl\", \"wb\"))\n",
    "pickle.dump(both_truncated_scores, open(f\"../data/{base_model}_CE_truncated_scores.pkl\", \"wb\"))\n",
    "pickle.dump(doc_truncated_scores, open(f\"../data/{base_model}_CE_doc_truncated_scores.pkl\", \"wb\"))\n",
    "pickle.dump(wiki_truncated_scores, open(f\"../data/{base_model}_CE_wiki_truncated_scores.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6440703a-0db9-4790-9b39-88af69b9fef6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T09:53:22.500233Z",
     "iopub.status.busy": "2022-06-10T09:53:22.499405Z",
     "iopub.status.idle": "2022-06-10T09:53:22.542288Z",
     "shell.execute_reply": "2022-06-10T09:53:22.541110Z",
     "shell.execute_reply.started": "2022-06-10T09:53:22.500142Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = \"cross-encoder/stsb-roberta-base\"\n",
    "\n",
    "sim_matrixes = pickle.load(open(f\"../data/{base_model}_CE_scores.pkl\", \"rb\"))\n",
    "both_truncated_scores = pickle.load(open(f\"../data/{base_model}_CE_truncated_scores.pkl\", \"rb\"))\n",
    "doc_truncated_scores = pickle.load(open(f\"../data/{base_model}_CE_doc_truncated_scores.pkl\", \"rb\"))\n",
    "wiki_truncated_scores = pickle.load(open(f\"../data/{base_model}_CE_wiki_truncated_scores.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c201c3-5758-45f7-a09e-7f7a490de49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best wikipedia article match for this url and use it\n",
    "# get best PARARAPGRAH and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f92ab2d5-c66a-4233-9bcc-84e4c29c15fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T10:50:39.824329Z",
     "iopub.status.busy": "2022-06-10T10:50:39.823638Z",
     "iopub.status.idle": "2022-06-10T10:50:40.081358Z",
     "shell.execute_reply": "2022-06-10T10:50:40.080656Z",
     "shell.execute_reply.started": "2022-06-10T10:50:39.824269Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59f810a51b5492d9acb11f973829613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/947 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select best first (1)\n",
    "# First, get the best wikipedia article by either averaging (mean) over the score of all sentences\n",
    "# Or getting the one with the higher value overall (max)\n",
    "best_wiki_mean_mean = {}  # Best wiki paragraph by mean and average over its scores\n",
    "best_wiki_mean_max = {}  # Best wiki paragraph by mean and MAX of its scores\n",
    "best_wiki_max_mean = {}  # Best wiki paragraph by MAX and mean over its scores\n",
    "\n",
    "# (2)\n",
    "best_sentence_mean_mean = {}\n",
    "best_sentence_max_mean = {}\n",
    "best_sentence_mean_max = {}\n",
    "\n",
    "# best overall (3)\n",
    "best_pairwise = {}\n",
    "\n",
    "# truncated variations\n",
    "max_wiki_truncated = {}  # MAX over the scores of the doc sentences over the truncated wiki\n",
    "mean_wiki_truncated = {}  # MEAN over the scores of the doc sentences over the truncated wiki\n",
    "max_sentence_truncated = {}  # MAX over the scores of the WIKI sentences over the truncated DOC\n",
    "mean_sentence_truncated = {}  # MEAN over the scores of the WIKI sentences over the truncated DOC\n",
    "both_truncated = both_truncated_scores\n",
    "\n",
    "\n",
    "for line in tqdm(open(\"../data/clicked_docs_with_topics.tsv\"), total=947):\n",
    "    try:\n",
    "        url, topic, text = line.strip().split(\"\\t\", maxsplit=2)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    sim_matrix = sim_matrixes[url]\n",
    "\n",
    "    # Best first (1)\n",
    "    best_wiki_sentence = sim_matrix[:, np.argmax(np.sum(sim_matrix, axis=0))]\n",
    "    best_wiki_mean_mean[url] = np.mean(best_wiki_sentence)\n",
    "    best_wiki_mean_max[url] = np.max(best_wiki_sentence)\n",
    "\n",
    "    best_wiki_sentence = sim_matrix[:, np.argmax(np.sum(sim_matrix, axis=0))]\n",
    "    best_wiki_max_mean[url] = np.mean(best_wiki_sentence)\n",
    "\n",
    "    # (2)\n",
    "    best_doc_sentence = sim_matrix[np.argmax(np.sum(sim_matrix, axis=1))]\n",
    "    best_sentence_mean_mean[url] = np.mean(best_doc_sentence)\n",
    "    best_sentence_mean_max[url] = np.max(best_doc_sentence)\n",
    "\n",
    "    best_doc_sentence = sim_matrix[np.argmax(np.max(sim_matrix, axis=1))]\n",
    "    best_sentence_max_mean[url] = np.mean(best_doc_sentence)\n",
    "\n",
    "    # (3)\n",
    "    best_pairwise[url] = np.max(sim_matrix)\n",
    "\n",
    "    # (4)\n",
    "    max_wiki_truncated[url] = np.max(wiki_truncated_scores[url])\n",
    "    mean_wiki_truncated[url] = np.mean(wiki_truncated_scores[url])\n",
    "    max_sentence_truncated[url] = np.max(doc_truncated_scores[url])\n",
    "    mean_sentence_truncated[url] = np.mean(doc_truncated_scores[url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3cf4806-d66e-4ed0-879d-dec38954fe48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T11:12:02.048906Z",
     "iopub.status.busy": "2022-06-10T11:12:02.048241Z",
     "iopub.status.idle": "2022-06-10T11:12:02.088369Z",
     "shell.execute_reply": "2022-06-10T11:12:02.087265Z",
     "shell.execute_reply.started": "2022-06-10T11:12:02.048848Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m_name = base_model.split(\"/\")[1]\n",
    "pickle.dump(best_wiki_mean_mean, open(f\"../data/cross_encoder_scores/{m_name}_best_wiki_mean_mean.pkl\", \"wb\"))\n",
    "pickle.dump(best_wiki_mean_max, open(f\"../data/cross_encoder_scores/{m_name}_best_wiki_mean_max.pkl\", \"wb\"))\n",
    "pickle.dump(best_wiki_max_mean, open(f\"../data/cross_encoder_scores/{m_name}_best_wiki_max_mean.pkl\", \"wb\"))\n",
    "\n",
    "pickle.dump(best_sentence_mean_mean, open(f\"../data/cross_encoder_scores/{m_name}_best_sentence_mean_mean.pkl\", \"wb\"))\n",
    "pickle.dump(best_sentence_mean_max, open(f\"../data/cross_encoder_scores/{m_name}_best_sentence_mean_max.pkl\", \"wb\"))\n",
    "pickle.dump(best_sentence_max_mean, open(f\"../data/cross_encoder_scores/{m_name}_best_sentence_max_mean.pkl\", \"wb\"))\n",
    "\n",
    "pickle.dump(best_pairwise, open(f\"../data/cross_encoder_scores/{m_name}_best_pairwise.pkl\", \"wb\"))\n",
    "\n",
    "pickle.dump(max_wiki_truncated, open(f\"../data/cross_encoder_scores/{m_name}_max_wiki_truncated.pkl\", \"wb\"))\n",
    "pickle.dump(mean_wiki_truncated, open(f\"../data/cross_encoder_scores/{m_name}_mean_wiki_truncated.pkl\", \"wb\"))\n",
    "pickle.dump(mean_sentence_truncated, open(f\"../data/cross_encoder_scores/{m_name}_mean_sentence_truncated.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESIRES",
   "language": "python",
   "name": "desires"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0b217a5ac9dc4622b99373240079e5aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_100cf0c54e7d4c6490afaf67a65a5da4",
       "max": 1137,
       "style": "IPY_MODEL_c38af362e0e34292a160c24239b67ddd",
       "value": 1137
      }
     },
     "100cf0c54e7d4c6490afaf67a65a5da4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2452707ba93240b4bbac1e0f2eeace62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "30fbbaab89b7493b9e7cf9e1e9b777dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2452707ba93240b4bbac1e0f2eeace62",
       "style": "IPY_MODEL_7cf376a719fe42eeaa888058f64efd73",
       "value": " 1137/1137 [01:03&lt;00:00, 12.35it/s]"
      }
     },
     "605070b476d54ef5b6ba53ab726a9ad7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_f18e315cd85a43649f129d7a07f087b8",
        "IPY_MODEL_0b217a5ac9dc4622b99373240079e5aa",
        "IPY_MODEL_30fbbaab89b7493b9e7cf9e1e9b777dd"
       ],
       "layout": "IPY_MODEL_f0ff15a62197477aabcd01dffaaf822b"
      }
     },
     "7cf376a719fe42eeaa888058f64efd73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "895d8db019e142a9b7e635118d7a210c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "98f1239b4a6c4c3592ab69f7ef448477": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c38af362e0e34292a160c24239b67ddd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f0ff15a62197477aabcd01dffaaf822b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f18e315cd85a43649f129d7a07f087b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_98f1239b4a6c4c3592ab69f7ef448477",
       "style": "IPY_MODEL_895d8db019e142a9b7e635118d7a210c",
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
