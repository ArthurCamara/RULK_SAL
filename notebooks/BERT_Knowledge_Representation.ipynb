{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed1e2cf-cc71-4ea0-b9f9-d45c982a9934",
   "metadata": {},
   "source": [
    "## BERT Knowledge Representation\n",
    "We want to compute, at each step of the user session (i.e. each document clicked), how their internal knowledge representation changes. Therefore, we have a few different methods to do so using Bert embeddings as a starting point.\n",
    "A few assumptions:\n",
    "- user starts with an empty knowledge representation\n",
    "- User READS every document, and that is added to their knowledge\n",
    "\n",
    "Document embeddings for BERT can come in a few different forms. Check `Compute_BERT_embeddings.ipynb` for how we compute each:\n",
    "- SUM: sum of the embeddings for each sentence of the document\n",
    "- MEAN: mean of the embeddings for each sentence of the document\n",
    "- TRUNC: Truncate the document at the first 384 tokens.\n",
    "- maxp_pairwise: Considering all sentences from the Wikipedia topic and the document, consider only the sentence with the higher similarity for any Wikipedia sentence\n",
    "- maxp_sum: Consider only the sentence with higher similarity to the SUM of the wikipedia sentences\n",
    "- maxp_mean: Consider only the sentence with higher similarity to the MEAN of the wikipedia sentences\n",
    "- maxp_trunc: Consider only the sentence with higher similarity to the truncated wikipedia document\n",
    "    \n",
    "These are the ways we can compute the users' knowledge evolution. Will be compared to the same method of aggregation on the Wikipedia text\n",
    "\n",
    "- MEAN: Concatenate all of the documents, the MEAN of these is the final knowledge.\n",
    "- SUM: As the user clicks on documents, SUM the embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1493149-3d5a-4404-9e28-967253b01ba0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../docs_maxp_trunc_embeddings.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(v)\n\u001b[1;32m     13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/logs_with_position.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     16\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../docs_mean_embeddings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m: pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../docs_sum_embeddings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs_trunc\u001b[39m\u001b[38;5;124m\"\u001b[39m: pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../docs_trunc_embeddings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxp_pairwise\u001b[39m\u001b[38;5;124m\"\u001b[39m: pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../docs_maxp_pairwise_embeddings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxp_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m: pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../docs_maxp_sum_embeddings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxp_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../docs_maxp_mean_embeddings.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxp_trunc\u001b[39m\u001b[38;5;124m\"\u001b[39m: pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../docs_maxp_trunc_embeddings.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     24\u001b[0m }\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../docs_maxp_trunc_embeddings.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import urllib.parse\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def normalize_vector(v):\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "\n",
    "dataset = json.load(open(\"../data/logs_with_position.json\"))\n",
    "\n",
    "\n",
    "embeddings = {\n",
    "    \"docs_mean\": pickle.load(open(\"../data/docs_mean_embeddings.pkl\", \"rb\")),\n",
    "    \"docs_sum\": pickle.load(open(\"../data/docs_sum_embeddings.pkl\", \"rb\")),\n",
    "    \"docs_trunc\": pickle.load(open(\"../data/docs_trunc_embeddings.pkl\", \"rb\")),\n",
    "    \"maxp_pairwise\": pickle.load(open(\"../data/docs_maxp_pairwise_embeddings.pkl\", \"rb\")),\n",
    "    \"maxp_sum\": pickle.load(open(\"../data/docs_maxp_sum_embeddings.pkl\", \"rb\")),\n",
    "    \"maxp_mean\": pickle.load(open(\"../data/docs_maxp_mean_embeddings.pkl\", \"rb\")),\n",
    "    \"maxp_trunc\": pickle.load(open(\"../data/docs_maxp_trunc_embeddings.pkl\", \"rb\")),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83080e-a4a1-4254-8fdb-db42149ac37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(12)\n",
    "rolling_avg = 0.0\n",
    "\n",
    "\n",
    "for n, i in enumerate(a):\n",
    "    rolling_avg += (i - rolling_avg) / (n + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9bf919a-531e-4e80-96e7-c648f2ae9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = list(embeddings.keys())\n",
    "users_knowledge_MEAN = []  # add final score for the user\n",
    "users_knowledge_SUM = []\n",
    "\n",
    "missing_docs = set()\n",
    "\n",
    "for u in dataset:\n",
    "    user_knowledge_mean = {k: np.zeros(768) for k in methods}\n",
    "    user_knowledge_sum = {k: np.zeros(768) for k in methods}\n",
    "    clicks = 0\n",
    "    for d in u[\"clicks\"]:\n",
    "        url = d[\"url\"]\n",
    "        if url not in docs_mean or not np.any(embeddings[\"docs_mean\"][url]):\n",
    "            missing_docs.add(url)\n",
    "            continue\n",
    "        clicks += 1\n",
    "        for method in methods:\n",
    "            emb = embeddings[method][url]\n",
    "            user_knowledge_mean[method] += (emb - user_knowledge_mean[method]) / (clicks)\n",
    "            user_knowledge_sum[method] += emb  # normalize at the end\n",
    "    # normalize and compute final knowledge gain\n",
    "    for method in methods:\n",
    "        user_knowledge_mean[method] = normalize_vector(user_knowledge_mean[method])\n",
    "        user_knowledge_sum[method] = normalize_vector(user_knowledge_sum[method])\n",
    "        \n",
    "        \n",
    "        \n",
    "    # final scores\n",
    "    for method \n",
    "    \n",
    "    # print(u)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2644d-56be-4711-8553-5be992002194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESIRES",
   "language": "python",
   "name": "desires"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
