{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed1e2cf-cc71-4ea0-b9f9-d45c982a9934",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BERT Knowledge Representation\n",
    "We want to compute, at each step of the user session (i.e. each document clicked), how their internal knowledge representation changes. Therefore, we have a few different methods to do so using Bert embeddings as a starting point.\n",
    "A few assumptions:\n",
    "- user starts with an empty knowledge representation\n",
    "- User READS every document, and that is added to their knowledge\n",
    "\n",
    "Document embeddings for BERT can come in a few different forms. Check `Compute_BERT_embeddings.ipynb` for how we compute each:\n",
    "- SUM: sum of the embeddings for each sentence of the document\n",
    "- MEAN: mean of the embeddings for each sentence of the document\n",
    "- TRUNC: Truncate the document at the first 384 tokens.\n",
    "- maxp_pairwise: Considering all sentences from the Wikipedia topic and the document, consider only the sentence with the higher similarity for any Wikipedia sentence\n",
    "- maxp_sum: Consider only the sentence with higher similarity to the SUM of the wikipedia sentences\n",
    "- maxp_mean: Consider only the sentence with higher similarity to the MEAN of the wikipedia sentences\n",
    "- maxp_trunc: Consider only the sentence with higher similarity to the truncated wikipedia document\n",
    "    \n",
    "These are the ways we can compute the users' knowledge evolution. Will be compared to the same method of aggregation on the Wikipedia text\n",
    "\n",
    "- MEAN: Concatenate all of the documents, the MEAN of these is the final knowledge.\n",
    "- SUM: As the user clicks on documents, SUM the embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24451295-8c9c-4e34-be90-7b3a2e50d9dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T14:40:57.781259Z",
     "iopub.status.busy": "2022-06-24T14:40:57.780619Z",
     "iopub.status.idle": "2022-06-24T14:40:57.791353Z",
     "shell.execute_reply": "2022-06-24T14:40:57.789980Z",
     "shell.execute_reply.started": "2022-06-24T14:40:57.781201Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import urllib.parse\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1493149-3d5a-4404-9e28-967253b01ba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T14:40:57.939323Z",
     "iopub.status.busy": "2022-06-24T14:40:57.938974Z",
     "iopub.status.idle": "2022-06-24T14:40:57.967226Z",
     "shell.execute_reply": "2022-06-24T14:40:57.966468Z",
     "shell.execute_reply.started": "2022-06-24T14:40:57.939294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalized = True\n",
    "base_model, hidden_size = \"msmarco-MiniLM-L6-cos-v5\", 384\n",
    "\n",
    "# base_model, hidden_size = \"all-MiniLM-L6-v2\", 384\n",
    "# base_model, hidden_size = \"all-mpnet-base-v2\", 768\n",
    "f_dist = cosine\n",
    "\n",
    "\n",
    "def load_embeddings(name, doc=\"docs\", normalized=normalized):\n",
    "    if normalized:\n",
    "        if doc==\"wikipedia\":\n",
    "            return pickle.load(open(f\"../data/bert_embeddings/{base_model}_{doc}_{name}_embeddings_norm.pkl\", \"rb\"))\n",
    "        return pickle.load(open(f\"../data/bert_embeddings/{base_model}_{doc}_{name}_embeddings_norm.pkl\", \"rb\"))\n",
    "\n",
    "    if doc==\"wikipedia\":\n",
    "        return pickle.load(open(f\"../data/bert_embeddings/{base_model}_{doc}_{name}_embeddings.pkl\", \"rb\"))\n",
    "    return pickle.load(open(f\"../data/bert_embeddings/{base_model}_{doc}_{name}_embeddings.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "dataset = json.load(open(\"../data/logs_with_position.json\"))\n",
    "\n",
    "embeddings = {\n",
    "    \"docs_mean\": load_embeddings(\"mean\"),\n",
    "    \"docs_sum\": load_embeddings(\"sum\"),\n",
    "    \"docs_trunc\": load_embeddings(\"trunc\"),\n",
    "}\n",
    "\n",
    "wikipedia_embeddings = {\n",
    "    \"sum\": load_embeddings(\"sum\", \"wikipedia\"),\n",
    "    \"mean\": load_embeddings(\"mean\", \"wikipedia\"),\n",
    "    \"trunc\": load_embeddings(\"trunc\", \"wikipedia\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9bf919a-531e-4e80-96e7-c648f2ae9b66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T14:41:28.691343Z",
     "iopub.status.busy": "2022-06-24T14:41:28.690658Z",
     "iopub.status.idle": "2022-06-24T14:41:29.051019Z",
     "shell.execute_reply": "2022-06-24T14:41:29.050230Z",
     "shell.execute_reply.started": "2022-06-24T14:41:28.691282Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "methods = list(embeddings.keys())\n",
    "users_knowledge_MEAN = []  # add final score for the user\n",
    "users_knowledge_SUM = []\n",
    "final_knowledges = defaultdict(OrderedDict)\n",
    "missing_docs = set()\n",
    "\n",
    "for u in dataset:\n",
    "    u_id = u[\"userID\"]\n",
    "    ALG = u[\"ALG\"]\n",
    "    RPL = u[\"RPL\"]\n",
    "\n",
    "    user_knowledge_mean = {k: [] for k in methods}\n",
    "    user_knowledge_sum = {k: np.zeros(hidden_size) for k in methods}\n",
    "    topic = urllib.parse.quote(u[\"topic_title\"])\n",
    "    clicks = 0\n",
    "    for d in u[\"clicks\"]:\n",
    "        url = d[\"url\"]\n",
    "        if url not in embeddings[\"docs_mean\"] or not np.any(embeddings[\"docs_mean\"][url]):\n",
    "            missing_docs.add(url)\n",
    "            continue\n",
    "        clicks += 1\n",
    "        for method in methods:\n",
    "            emb = embeddings[method][url]\n",
    "            user_knowledge_mean[method].append(emb)\n",
    "            user_knowledge_sum[method] += emb\n",
    "    # normalize and compute final similarity\n",
    "    for method in methods:\n",
    "        if normalized:\n",
    "            knowledge_mean = normalize(np.mean(user_knowledge_mean[method], axis=0).reshape(1, -1)).flatten()\n",
    "            knowledge_sum = normalize(user_knowledge_sum[method].reshape(1, -1)).flatten()\n",
    "        else:\n",
    "            knowledge_mean = np.mean(user_knowledge_mean[method], axis=0)\n",
    "            knowledge_sum = user_knowledge_sum[method]\n",
    "\n",
    "        for emb_type in wikipedia_embeddings.keys():\n",
    "            wiki_emb = wikipedia_embeddings[emb_type][topic]\n",
    "            sum_dict = {\"RPL\": RPL, \"ALG\": ALG, \"final_sim\": 1 - f_dist(knowledge_sum, wiki_emb)}\n",
    "            mean_dict = {\"RPL\": RPL, \"ALG\": ALG, \"final_sim\": 1 - f_dist(knowledge_mean, wiki_emb)}\n",
    "\n",
    "            final_knowledges[f\"SUM_{method}_wiki_{emb_type}\"][u_id] = sum_dict\n",
    "            final_knowledges[f\"MEAN_{method}_wiki_{emb_type}\"][u_id] = mean_dict\n",
    "\n",
    "pickle.dump(dict(final_knowledges), open(f\"../data/{base_model}_knowledge_gains.pkl\", \"wb\"))\n",
    "with open(\"../data/missing_docs.txt\", \"w\") as outf:\n",
    "    for u in missing_docs:\n",
    "        outf.write(f\"{u}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ef2c9-f4e6-4a9c-9c34-2c9f2c37731f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Correlation between learning and embedding methods\n",
    "\n",
    "In the end, we want to know, for each of the 42 methods tried above, which one has the higher correlation with learning gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cd21141-9100-422e-a9cb-bfe34fde09c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T14:41:32.504959Z",
     "iopub.status.busy": "2022-06-24T14:41:32.504372Z",
     "iopub.status.idle": "2022-06-24T14:41:32.562213Z",
     "shell.execute_reply": "2022-06-24T14:41:32.561260Z",
     "shell.execute_reply.started": "2022-06-24T14:41:32.504903Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msmarco-MiniLM-L6-cos-v5\n",
      "[('MEAN_docs_sum_wiki_sum', (0.2728320613235774, 0.0019133811815497507)),\n",
      " ('MEAN_docs_sum_wiki_mean', (0.2728320205832326, 0.0019133842969220943)),\n",
      " ('SUM_docs_mean_wiki_mean', (0.27283201127233553, 0.0019133850089174923))]\n",
      "[('SUM_docs_mean_wiki_sum',\n",
      "  SpearmanrResult(correlation=0.3152560538984141, pvalue=0.00030603509392881174)),\n",
      " ('MEAN_docs_mean_wiki_sum',\n",
      "  SpearmanrResult(correlation=0.3152560538984141, pvalue=0.00030603509392881174)),\n",
      " ('SUM_docs_mean_wiki_mean',\n",
      "  SpearmanrResult(correlation=0.3152560538984141, pvalue=0.00030603509392881174))]\n"
     ]
    }
   ],
   "source": [
    "RPLs = []\n",
    "ALGs = []\n",
    "u_ids = [u[\"userID\"] for u in dataset]\n",
    "\n",
    "# make sure we follow the same order of users\n",
    "pearsons_ALG = {}\n",
    "spearman_ALG = {}\n",
    "kendalltau_ALG = {}\n",
    "corr_ALG = {}\n",
    "\n",
    "for m in final_knowledges:\n",
    "    u_ids = final_knowledges[m].keys()\n",
    "    ALGs = [final_knowledges[m][u][\"ALG\"] for u in u_ids]\n",
    "    RPLs = [final_knowledges[m][u][\"RPL\"] for u in u_ids]\n",
    "    results = [final_knowledges[m][u][\"final_sim\"] for u in u_ids]\n",
    "\n",
    "    # results = [_users[x][\"final_sim\"] for x in u_ids if x in _users]\n",
    "    pearsons_ALG[m] = pearsonr(results, RPLs)\n",
    "    spearman_ALG[m] = spearmanr(results, RPLs)\n",
    "\n",
    "    pearsons_ALG[m] = pearsonr(results, ALGs)\n",
    "    spearman_ALG[m] = spearmanr(results, ALGs)\n",
    "\n",
    "\n",
    "print(base_model)\n",
    "pprint(sorted(pearsons_ALG.items(), key=lambda x: x[1], reverse=True)[:3])\n",
    "pprint(sorted(spearman_ALG.items(), key=lambda x: x[1], reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5222c-5b39-4375-ac00-5708129a4580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESIRES",
   "language": "python",
   "name": "desires"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
